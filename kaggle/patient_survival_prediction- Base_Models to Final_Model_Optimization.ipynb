{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/ngtung/opt/anaconda3/lib/python3.9/site-packages (1.7.4)\r\n",
      "Requirement already satisfied: numpy in /Users/ngtung/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.21.5)\r\n",
      "Requirement already satisfied: scipy in /Users/ngtung/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.9.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important libraries\n",
    "#importing the necessary packages\n",
    "import pandas as pd\n",
    "import string\n",
    "#import plotly.express as px\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import zscore\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder,PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,log_loss,classification_report\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV,KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Dataset.csv/Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rm/dpjxptpx1jj__fvjkcmq6svc0000gn/T/ipykernel_10860/1179007474.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpatient_records\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Dataset.csv/Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpatient_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Dataset.csv/Dataset.csv'"
     ]
    }
   ],
   "source": [
    "patient_records=pd.read_csv('/Dataset.csv/Dataset.csv')\n",
    "patient_records.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look at the Dataset Descriptions below:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Description=pd.read_csv('Data Dictionary.csv')\n",
    "Description.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output variable (desired target):\n",
    " hospital_death- patient's survival outcomes lets take 0 as survive and 1 as not surive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of the dataset\n",
    "patient_records.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We have 91713 records in our dataset with 186 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exact all the column from dataset\n",
    "patient_records.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_missing_values = patient_records.isna().sum().sort_values(ascending=False) \n",
    "number_of_missing_values.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get in to the data information with datatypes\n",
    "patient_records.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check out the individual data types\n",
    "patient_records.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets check for the different data type\n",
    "binary_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_variables = [\n",
    "    \"hospital_death\", \n",
    "    \"elective_surgery\", \n",
    "    \"readmission_status\", \n",
    "    \"apache_post_operative\", \n",
    "    \"arf_apache\", \n",
    "    \"gcs_unable_apache\", \n",
    "    \"intubated_apache\", \n",
    "    \"ventilated_apache\", \n",
    "    \"aids\", \n",
    "    \"cirrhosis\", \n",
    "    \"diabetes_mellitus\", \n",
    "    \"hepatic_failure\", \n",
    "    \"immunosuppression\", \n",
    "    \"leukemia\", \n",
    "    \"lymphoma\", \n",
    "    \"solid_tumor_with_metastasis\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[binary_variables].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[binary_variables].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now look for the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[binary_variables].isnull().sum() * 100 / patient_records[binary_variables].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some columns containing the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_convert_variables = [ \n",
    "    \"arf_apache\", \n",
    "    \"gcs_unable_apache\", \n",
    "    \"intubated_apache\", \n",
    "    \"ventilated_apache\", \n",
    "    \"aids\", \n",
    "    \"cirrhosis\", \n",
    "    \"diabetes_mellitus\", \n",
    "    \"hepatic_failure\", \n",
    "    \"immunosuppression\", \n",
    "    \"leukemia\", \n",
    "    \"lymphoma\", \n",
    "    \"solid_tumor_with_metastasis\"\n",
    "]\n",
    "# patient_records[\"arf_apache\"] = patient_records[\"arf_apache\"].fillna(patient_records[\"arf_apache\"].mode())\n",
    "for var in to_convert_variables:\n",
    "    print(var)\n",
    "    print(patient_records[var].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[to_convert_variables] = patient_records[to_convert_variables].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[to_convert_variables].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_records[\"arf_apache\"].astype(np.int64)\n",
    "patient_records['arf_apache'] = patient_records['arf_apache'].apply(np.int64)\n",
    "patient_records['gcs_unable_apache'] = patient_records['gcs_unable_apache'].apply(np.int64)\n",
    "patient_records['intubated_apache'] = patient_records['intubated_apache'].apply(np.int64)\n",
    "patient_records['ventilated_apache'] = patient_records['ventilated_apache'].apply(np.int64)\n",
    "patient_records['aids'] = patient_records['aids'].apply(np.int64)\n",
    "patient_records['cirrhosis'] = patient_records['cirrhosis'].apply(np.int64)\n",
    "patient_records['diabetes_mellitus'] = patient_records['diabetes_mellitus'].apply(np.int64)\n",
    "patient_records['hepatic_failure'] = patient_records['hepatic_failure'].apply(np.int64)\n",
    "patient_records['immunosuppression'] = patient_records['immunosuppression'].apply(np.int64)\n",
    "patient_records['leukemia'] = patient_records['leukemia'].apply(np.int64)\n",
    "patient_records['lymphoma'] = patient_records['lymphoma'].apply(np.int64)\n",
    "patient_records['solid_tumor_with_metastasis'] = patient_records['solid_tumor_with_metastasis'].apply(np.int64)\n",
    "patient_records[binary_variables].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.drop([\"apache_post_operative\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Interpretation: </strong> <br>\n",
    "- We can see that the target variable hospital death is of binary type. It also does not contain any null values. Thus no furthur nmodifications required for the target variable.<br>\n",
    "- Checking the null values, it is seen that only around 0.7 - 1 % of values are missing. Hence they are not dropped.<br>\n",
    "- But dropping feature apache_post_operative\n",
    "- Using mode, adding the missing values. Converting all to int type\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## catergorical variable\n",
    "which containing the object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as by default our target variable is a numerical data we will change our datatype to categorical \n",
    "patient_records['hospital_death']=patient_records['hospital_death'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_catgeorical = patient_records.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_catgeorical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_catgeorical.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_catgeorical.isnull().sum() * 100 / patient_catgeorical.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\"gender\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\",\"ethnicity\",\"hospital_admit_source\",\"icu_admit_source\"]:\n",
    "    print(var)\n",
    "    print(patient_records[var].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[\"gender\"] = patient_records[\"gender\"].fillna(\"M\")\n",
    "patient_records[\"apache_3j_bodysystem\"] = patient_records[\"apache_3j_bodysystem\"].fillna(\"Cardiovascular\")\n",
    "patient_records[\"apache_2_bodysystem\"] = patient_records[\"apache_2_bodysystem\"].fillna(\"Cardiovascular\")\n",
    "patient_records[\"ethnicity\"] = patient_records[\"ethnicity\"].fillna(\"Caucasian\")\n",
    "patient_records[\"hospital_admit_source\"] = patient_records[\"hospital_admit_source\"].fillna(\"Floor\")\n",
    "patient_records[\"icu_admit_source\"] = patient_records[\"icu_admit_source\"].fillna(\"Floor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\"gender\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\",\"ethnicity\",\"hospital_admit_source\",\"icu_admit_source\"]:\n",
    "    print(patient_records[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.apache_2_bodysystem.replace({\"Undefined diagnoses\": \"Undefined Diagnoses\"}, inplace=True)\n",
    "for var in [\"gender\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\"]:\n",
    "    print(patient_records[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_catgeorical = patient_records.select_dtypes(object)\n",
    "patient_catgeorical.isnull().sum() * 100 / patient_catgeorical.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Interpretation: </strong> <br>\n",
    "- These are our categorical variables with many categories.<br>\n",
    "- We have found count of categories for each feature.<br>\n",
    "- Checking the null values, it is seen that only around 1.5 - 24 % of values are missing.<br>\n",
    "- From the information, other than gender, icu_type and APACHE bodysystem, the other features .\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete variables containing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_variables = [\n",
    "    \"encounter_id\", \n",
    "    \"hospital_id\", \n",
    "    \"patient_id\", \n",
    "    \"icu_id\", \n",
    "    \"gcs_eyes_apache\", \n",
    "    \"gcs_motor_apache\", \n",
    "    \"gcs_verbal_apache\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_discrete = patient_records[integer_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_discrete.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_discrete.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_discrete.isnull().sum() * 100 / patient_discrete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_discrete[\"gcs_motor_apache\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_discrete[\"gcs_eyes_apache\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_discrete[\"gcs_verbal_apache\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets drop some column which doesnt show any relation with other column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.drop([\"encounter_id\", \"hospital_id\", \"icu_id\", \"patient_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['gcs_motor_apache', 'gcs_eyes_apache', 'gcs_verbal_apache']:\n",
    "    print(var)\n",
    "    print(patient_records[var].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[\"gcs_motor_apache\"] = patient_records[\"gcs_motor_apache\"].fillna(6)\n",
    "patient_records[\"gcs_eyes_apache\"] = patient_records[\"gcs_eyes_apache\"].fillna(4)\n",
    "patient_records[\"gcs_verbal_apache\"] = patient_records[\"gcs_verbal_apache\"].fillna(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records['gcs_motor_apache'] = patient_records['gcs_motor_apache'].apply(np.int64)\n",
    "patient_records['gcs_eyes_apache'] = patient_records['gcs_eyes_apache'].apply(np.int64)\n",
    "patient_records['gcs_verbal_apache'] = patient_records['gcs_verbal_apache'].apply(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records[['gcs_motor_apache', 'gcs_eyes_apache', 'gcs_verbal_apache']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Interpretation: </strong> <br>\n",
    "- These are our discrete variables.<br>\n",
    "- From the information, the ids such as patient_id, hospital_id, icu_id, encounter_id do not seem to contribute to the cause of death at the hospital., Thus we can drop these columns<br>\n",
    "- The APACHE readings in this list seem to be discrete but in a categorical manner. Hence it will be added to the category variable with many categories.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look for the Continous numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous = patient_records.select_dtypes(np.float64)\n",
    "patient_continuous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_basis_readings = [i for i in patient_continuous.columns.tolist() if i.startswith(\"h1\")]\n",
    "len(hour_basis_readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looking for the patients with hour basis reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous[hour_basis_readings].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous[hour_basis_readings].isnull().sum() * 100 / patient_continuous[hour_basis_readings].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Interpretation: </strong> <br>\n",
    "- These are the observations within 24 hours<br>\n",
    "- Most of these have null values, and do not seem to give any contributions to the hospital death analysis. Hence dropped.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now look for the patients with day basis reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_basis_readings = [i for i in patient_continuous.columns.tolist() if i.startswith(\"d1\")]\n",
    "len(day_basis_readings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous[day_basis_readings].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous[day_basis_readings].isnull().sum() * 100 / patient_continuous[day_basis_readings].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Interpretation: </strong> <br>\n",
    "- These are the observations within 1 day<br>\n",
    "- Most of these have null values, and do not seem to give any contributions to the hospital death analysis. Hence dropped.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.drop(hour_basis_readings, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.drop(day_basis_readings, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous = patient_records.select_dtypes(np.float64)\n",
    "patient_continuous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous.isnull().sum() * 100 / patient_continuous.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\"albumin_apache\", \"bilirubin_apache\", \"fio2_apache\", \"paco2_apache\", \"paco2_for_ph_apache\", \"pao2_apache\", \"ph_apache\", \"urineoutput_apache\"]\n",
    "patient_records.drop(to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.drop(\"pre_icu_los_days\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Interpretation: </strong> <br>\n",
    "- The columns with more than 50% of missing values are dropped.<br>\n",
    "- The columns pre_icu_los_days is also dropped as we felt its not required.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous = patient_records.select_dtypes(np.float64)\n",
    "patient_continuous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>Interpretation: </strong> <br>\n",
    "- Do not see any irregularites in observations, mainly for age, bmi, height, and weight. The minimum values do not have 0, so we can assume that even with null values, the observations are correct<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lets check it by skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patient_continuous.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_continuous.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill the null values\n",
    "\n",
    "#### Skewed Variable\n",
    "\n",
    "age, bmi, weight, apache_2_diagnosis, apache_3j_diagnosis, bun_apache, creatinine_apache, glucose_apache, map_apache, temp_apache, wbc_apache, apache_4a_hospital_death_prob, apache_4a_icu_death_prob\n",
    "\n",
    "\n",
    "#### Symmetric Variables\n",
    "\n",
    "height, heart_rate_apache, hematocrit_apache, resprate_apache, sodium_apache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_median = [\"age\", \"bmi\", \"weight\", \"apache_2_diagnosis\", \"apache_3j_diagnosis\", \"bun_apache\", \n",
    "                  \"creatinine_apache\", \"glucose_apache\", \"map_apache\", \"temp_apache\", \"wbc_apache\", \n",
    "                  \"apache_4a_hospital_death_prob\", \"apache_4a_icu_death_prob\"]\n",
    "for con in convert_median:\n",
    "    print(con)\n",
    "    mode = patient_records[con]\n",
    "#     patient_records[con].map({np.nan: mode})\n",
    "    patient_records[con].fillna(patient_records[con].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_median = [\"height\", \"heart_rate_apache\", \"hematocrit_apache\", \"resprate_apache\", \"sodium_apache\"]\n",
    "for con in convert_median:\n",
    "    print(con)\n",
    "    mode = patient_records[con]\n",
    "#     patient_records[con].map({np.nan: mode})\n",
    "    patient_records[con].fillna(patient_records[con].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFTER CLEANING THE DATASET BY REMOVING NULL VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's extract all the categorical and the numerical columns for the further Analysis\n",
    "df_categorical=patient_records.select_dtypes(object)\n",
    "print(df_categorical.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records.drop(['readmission_status'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical=patient_records.select_dtypes(include=np.number)\n",
    "df_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=df_categorical.columns\n",
    "numerical_columns=df_numerical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of categorical columns is: \",len(categorical_columns))\n",
    "print(\"The columns are:\",list(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of numerical columns is: \",len(numerical_columns))\n",
    "print(\"The columns are:\",list(numerical_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : We Have 9 Categorical Columns and 35 Numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing Exploratory Data Analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the null values \n",
    "number_of_missing_values = patient_records.isna().sum().sort_values(ascending=False)          \n",
    "percentage_of_missing_values = (patient_records.isna().sum()*100/patient_records.isna().count()).sort_values(ascending=False)   \n",
    "missing_data = pd.concat([number_of_missing_values,percentage_of_missing_values], axis =0 , keys = ['Total', 'Percentage of Missing Values'])    \n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows with missing values:\", patient_records.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = patient_records.isna().sum().sort_values(ascending=False)\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBSERVATION :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly observe from the above cell there are  no missing value present in many attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summerize the numerical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_records.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregating the continuous and categorical variables.\n",
    "df_numerical=patient_records.select_dtypes(np.number)\n",
    "df_categorical=patient_records.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the target variable from the dataframe:\n",
    "df_categorical=df_categorical.drop('hospital_death',axis=1)\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection And Treatment for Numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = patient_records.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the outliers by its skweness\n",
    "df_numerical.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,30))\n",
    "sns.heatmap(df_numerical.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "s=1\n",
    "for p in patient_continuous:\n",
    "    plt.subplot(6,3,s)\n",
    "    sns.distplot(patient_continuous[p])\n",
    "    s+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling/Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt=PowerTransformer()\n",
    "pwrtransformed_data_num=pd.DataFrame(pt.fit_transform(df_numerical),columns=df_numerical.columns)\n",
    "pwrtransformed_data_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the outliers by its skweness after power transformation:\n",
    "pwrtransformed_data_num.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Representation after Outlier Treatment :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##plt.figure(figsize=(50,20))\n",
    "s=1\n",
    "for p in patient_pwrtransformed_data_num:   \n",
    "    plt.subplot(6,5,s)\n",
    "    sns.distplot(patient_pwrtransformed_data_num[p])\n",
    "    s+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we have reduced the effect of outltiers and have prepared the final dataset for Predictive modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = patient_records.select_dtypes(object)\n",
    "df_categorical=df_categorical.drop('hospital_death',axis=1)\n",
    "Encode=pd.get_dummies(data=df_categorical,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encode=pd.get_dummies(patient_records,columns = ['hospital_death'],drop_first=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_records['hospital_death']=patient_records['hospital_death'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit Model analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variables:\n",
    "X=pwrtransformed_data_num\n",
    "\n",
    "# Target variable:\n",
    "y=patient_records[['hospital_death']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating the powertransformed numerical features with One hot encoded categorical featues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encode=pd.get_dummies(data=df_categorical,drop_first=True)\n",
    "\n",
    "# Concatenating the Encoded categorical attributes to the powertransformed numerical attributes:\n",
    "X=pd.concat([pd.DataFrame(X),Encode],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineeering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing features involved in multicollinearity uisng Variance Inflation Factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df_numerical.iloc[:,:-1]\n",
    "calc_vif(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X.columns)):\n",
    "    vif=pd.DataFrame()\n",
    "    vif['VIF Factor']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n",
    "    vif['Features']=X.columns\n",
    "    multi=vif[vif['VIF Factor']>10]\n",
    "    if (multi.empty==False):\n",
    "        df_numerical_sorted=multi.sort_values(by='VIF Factor',ascending=False)\n",
    "    else:\n",
    "        print(vif)\n",
    "        break\n",
    "    if (df_numerical_sorted.empty==False):\n",
    "        X=X.drop(df_numerical_sorted['Features'].iloc[0],axis=1)\n",
    "    else:\n",
    "        print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the dependent dataset\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing set in the ratio (80:20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc=10\n",
    "X=sm.add_constant(X)   # Initialising the random state at 10.\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit model summary for full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using statsmodels:\n",
    "X_train_const=sm.add_constant(X_train)\n",
    "logit_model=sm.Logit(y_train,X_train_const).fit()\n",
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of the features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significant features affecting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the p value for Wald test statistic is less than 0.05, we are considering the feature to be significant.\n",
    "significant=logit_model.pvalues[logit_model.pvalues<0.05]\n",
    "significant_features=pd.DataFrame(data=significant,columns=['p-value'])\n",
    "significant_features.drop('const',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the following dependent variables contribute significantly to the overall model performance:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log odds value for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds=round(logit_model.params,2)\n",
    "log_odds_data=pd.DataFrame(log_odds,columns=['Log odds'])\n",
    "log_odds_data.drop('const',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Odds Value for each feature:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds=round(np.exp(logit_model.params),2)\n",
    "odds_data=pd.DataFrame(odds,columns=['Odds value'])\n",
    "odds_data.sort_values(by='Odds value',ascending=False).drop('const',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher the odds value, greater is the relation of the feature with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation : McFadden's R^2 value(Pseudo R^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=logit_model.prsquared\n",
    "print('Logit model accuracy : ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy using Confusion Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the probability threshold to be 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the probability threshold:\n",
    "th=0.5\n",
    "ypred_prob=logit_model.predict(X_test)\n",
    "ypred=[0 if p<th else 1 for p in ypred_prob]\n",
    "\n",
    "# Computing the Confusion matrix:\n",
    "confusion_mat=confusion_matrix(y_test,ypred)\n",
    "tn = confusion_mat[0,0]\n",
    "tp = confusion_mat[1,1]\n",
    "fp = confusion_mat[0,1]\n",
    "fn = confusion_mat[1,0]\n",
    "print('Confusion Matrix for Logit Model : ')\n",
    "print('---------------------------------','\\n')\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical Representation of Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(confusion_mat,annot=True,linewidth=3.5,linecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating optimal value threshold for full model using the Youden's index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the True Positive rate(TPR),False Positive rate(FPR) and probability threshold:\n",
    "fpr, tpr, th= roc_curve(y_test, ypred_prob)\n",
    "\n",
    "# Creating a Dataframe of the data:\n",
    "youden= pd.DataFrame({'TPR': tpr,\n",
    "                       'FPR': fpr,\n",
    "                        'thres':th})\n",
    "\n",
    "# Calculating the Younden's index:\n",
    "youden['YI']= youden.TPR-youden.FPR\n",
    "youden= youden.sort_values(by='YI',ascending=False).reset_index(drop=True)\n",
    "print(youden.head(5))\n",
    "print('-----------------------------------------','\\n')\n",
    "# or\n",
    "print('Optimal threshold for probability: ',round(youden['YI'].max(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the Optimal probability threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the optimal probability threshold:\n",
    "opt_th=0.51\n",
    "ypred_prob=logit_model.predict(X_test)\n",
    "ypred_optimal=[0 if p<opt_th else 1 for p in ypred_prob]\n",
    "\n",
    "# Computing the Confusion matrix:\n",
    "confusion_mat_optimal=confusion_matrix(y_test,ypred_optimal)\n",
    "tn = confusion_mat_optimal[0,0]\n",
    "tp = confusion_mat_optimal[1,1]\n",
    "fp = confusion_mat_optimal[0,1]\n",
    "fn = confusion_mat_optimal[1,0]\n",
    "print('Confusion Matrix from Logit Model : ')\n",
    "print('------------------------------------','\\n')\n",
    "print(confusion_mat_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical Representation of Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(confusion_mat_optimal,annot=True,linewidth=3.5,linecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit model accuracy from confusion matrix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For probability threshold as 0.5:\n",
    "logit_model_accuracy_test=accuracy_score(y_test,ypred)\n",
    "print('Logit model accuracy for test data using confusion matrix : ',logit_model_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For optimal probability threshold as 0.51:\n",
    "logit_model_optimal_accuracy=accuracy_score(y_test,ypred_optimal)\n",
    "print('Logit model accuracy using confusion matrix : ',logit_model_optimal_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence we will be considering 0.51 as the threshold as it gives a slightly better overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit model classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the classification report:\n",
    "\n",
    "logit_classification=classification_report(y_test,ypred)\n",
    "print('Logit model classification report: ')\n",
    "print('------------------------------------','\\n')\n",
    "print(logit_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression Model using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model using sklearn:\n",
    "log_reg=LogisticRegression()\n",
    "log_model=log_reg.fit(X_train,y_train)\n",
    "ypred_lr_test=log_model.predict(X_test)\n",
    "ypred_lr_train=log_model.predict(X_train)\n",
    "ypred_lr_probability=log_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Model Performance Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).1.  Confusion Matrix for Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Confusion matrix: \n",
    "\n",
    "confusion_mat=confusion_matrix(y_test,ypred_lr_test)\n",
    "tn = confusion_mat[0,0]\n",
    "tp = confusion_mat[1,1]\n",
    "fp = confusion_mat[0,1]\n",
    "fn = confusion_mat[1,0]\n",
    "print('Confusion Matrix form Logistic Regression Model : ')\n",
    "print('------------------------------------------------','\\n')\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).2.  Graphical Representation of Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(confusion_mat,annot=True,linewidth=3.5,linecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity and Specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensitivity_Logistic=(tp/(tp+fn))\n",
    "print('Sensitivity of the Logistic Regression Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Sensitivity_Logistic)\n",
    "print('\\n')\n",
    "Specificity_Logistic=(tn/(tn+fp))\n",
    "print('Specificity of the Logistic Regression Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Specificity_Logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).3.  Accuracy Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Accuracy score of the test data for Logistic Regression model : ')\n",
    "print(accuracy_score(y_test,ypred_lr_test),'\\n')\n",
    "print('The Accuracy score of the train data for Logistic Regression model : ')\n",
    "print(accuracy_score(y_train,ypred_lr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).4. Logistic Regression model classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the classification report:\n",
    "\n",
    "logistic_report=classification_report(y_test,ypred_lr_test)\n",
    "print('Logit model classification report: ')\n",
    "print('------------------------------------','\\n')\n",
    "print(logistic_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).5. ROC Curve for Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,th=roc_curve(y_test,ypred_lr_probability)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr,tpr,color='green')\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.grid()\n",
    "plt.title('ROC curve for Logistic Regression Model')\n",
    "plt.plot([-0.05,1.05],[-0.05,1.05],'r--',linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).6. ROC AUC Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The ROC AUC score of the Logistic Regression model: ')\n",
    "print('--------------------------------------------------- ','\\n')\n",
    "print('ROC AUC : ',roc_auc_score(y_test,ypred_lr_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).7. Cross Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=log_loss(y_test,ypred_lr_test)\n",
    "print('The Cross Entropy score of the Logistic Regression model ')\n",
    "print('--------------------------------------------------- ','\\n')\n",
    "print('Cross Entropy : ',cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a).8. Bias Error and Variance Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "scores=cross_val_score(estimator=log_model,X=X_train,y=y_train,cv=k,scoring='accuracy')\n",
    "print(\"Mean Score : \",np.mean(scores))\n",
    "print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Inferences for Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross Entropy for the Logistic Regression Model is 2.77\n",
    "\n",
    "\n",
    "* ROC AUC Score for the Logistic Regression Model is 79.55\n",
    "\n",
    "\n",
    "* The Model Accuracy for the Logistic Regression Model is coming out to be around 92%.\n",
    "\n",
    "\n",
    "* f1 weighted avg for the Logistic Regression Model is around 90%.\n",
    "\n",
    "\n",
    "* Specificity : 98%\n",
    "\n",
    "\n",
    "* Sensitivity : 20.74%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-Nearest Neighbors Classification Model using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model using sklearn:\n",
    "knn_algorithm=KNeighborsClassifier()\n",
    "knn_model=knn_algorithm.fit(X_train,y_train)\n",
    "ypred_knn_test=knn_model.predict(X_test)\n",
    "ypred_knn_train=knn_model.predict(X_train)\n",
    "ypred_probability_knn=knn_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a) Model Performance Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).1. Confusion Matrix for K-Nearest Neighbors Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Confusion matrix:\n",
    "\n",
    "knn_confusion_mat=confusion_matrix(y_test,ypred_knn_test)\n",
    "tn = knn_confusion_mat[0,0]\n",
    "tp = knn_confusion_mat[1,1]\n",
    "fp = knn_confusion_mat[0,1]\n",
    "fn = knn_confusion_mat[1,0]\n",
    "print('Confusion Matrix for K-Nearest Neighbors Model : ')\n",
    "print('------------------------------------------------','\\n')\n",
    "print(knn_confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).2.  Graphical Representation of Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(knn_confusion_mat,annot=True,linewidth=3.5,linecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity and Specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensitivity_KNN=(tp/(tp+fn))\n",
    "print('Sensitivity of the KNN Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Sensitivity_KNN)\n",
    "print('\\n')\n",
    "Specificity_KNN=(tn/(tn+fp))\n",
    "print('Specificity of the KNN Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Specificity_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).3. Accuracy Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Accuracy score of the test data for K-Nearest Neighbors Model : ')\n",
    "print(accuracy_score(y_test,ypred_knn_test),'\\n')\n",
    "print('The Accuracy score of the train data for K-Nearest Neighbors Model : ')\n",
    "print(accuracy_score(y_train,ypred_knn_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).4. K-Nearest Neighbors Model classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the classification report:\n",
    "\n",
    "knn_report=classification_report(y_test,ypred_knn_test)\n",
    "print('K-Nearest Neighbors Model classification report: ')\n",
    "print('------------------------------------------------','\\n')\n",
    "print(knn_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).5. ROC Curve for K-Nearest Neighbors Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,th=roc_curve(y_test,ypred_probability_knn)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr,tpr,color='blue')\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.grid()\n",
    "plt.title('ROC curve for K-Nearest Neighbors Model')\n",
    "plt.plot([-0.05,1.05],[-0.05,1.05],'r--',linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).6. ROC AUC Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The ROC AUC score of the K-Nearest Neighbors Model ')\n",
    "print('--------------------------------------------------','\\n')\n",
    "print('ROC AUC : ',roc_auc_score(y_test,ypred_probability_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).7. Cross Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=log_loss(y_test,ypred_knn_test)\n",
    "print('The Cross Entropy score of K-Nearest Neighbors Model ')\n",
    "print('----------------------------------------------------','\\n')\n",
    "print('Cross Entropy : ',cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a).8. Bias Error and Variance Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "scores=cross_val_score(estimator=knn_model,X=X_train,y=y_train,cv=k,scoring='accuracy')\n",
    "print(\"Mean Score : \",np.mean(scores))\n",
    "print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b) Inferences for K-Nearest Neighbors Classification Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross Entropy for K-Nearest Neighbors Model is 3.10\n",
    "\n",
    "\n",
    "* ROC AUC Score for the K-Nearest Neighbors Model is 66.84 \n",
    "\n",
    "\n",
    "* The Model Accuracy for the K-Nearest Neighbors Model is coming out to be around 91%.\n",
    "\n",
    "\n",
    "* f1 weighted avg for the K-Nearest Neighbors Model is around 89%.\n",
    "\n",
    "\n",
    "* Specificity : 98.44%\n",
    "\n",
    "\n",
    "* Sensitivity : 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Classification Model using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model using sklearn:\n",
    "dt=DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "y_pred_test_dt=dt.predict(X_test)\n",
    "y_pred_train_dt=dt.predict(X_train)\n",
    "y_test_prob_dt=dt.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a) Model Performance Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).1. Confusion Matrix for Decision Tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Confusion matrix:\n",
    "\n",
    "dt_confusion_mat=confusion_matrix(y_test,y_pred_test_dt)\n",
    "tn = dt_confusion_mat[0,0]\n",
    "tp = dt_confusion_mat[1,1]\n",
    "fp = dt_confusion_mat[0,1]\n",
    "fn = dt_confusion_mat[1,0]\n",
    "print('Confusion Matrix for Decision Tree Model : ')\n",
    "print('------------------------------------------------','\\n')\n",
    "print(dt_confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).2. Graphical Representation of Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(dt_confusion_mat,annot=True,linewidth=3.5,linecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity and Specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensitivity_Decision_tree=(tp/(tp+fn))\n",
    "print('Sensitivity of the Decision Tree Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Sensitivity_Decision_tree)\n",
    "print('\\n')\n",
    "Specificity_Decision_tree=(tn/(tn+fp))\n",
    "print('Specificity of the Decision Tree Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Specificity_Decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).3.  Accuracy Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Accuracy score of test data for the Decision Tree Model: ')\n",
    "print(accuracy_score(y_test,y_pred_test_dt),'\\n')\n",
    "print('The Accuracy score of the train data for the Decision Tree Model: ')\n",
    "print(accuracy_score(y_train,y_pred_train_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).4.  Decision Tree Model classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing the classification report:\n",
    "\n",
    "dt_report=classification_report(y_test,y_pred_test_dt)\n",
    "print('Decision Tree Model classification report: ')\n",
    "print('-----------------------------------------','\\n')\n",
    "print(dt_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).5. ROC Curve for Decision Tree Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,th=roc_curve(y_test,y_test_prob_dt)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr,tpr,color='grey')\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.grid()\n",
    "plt.title('ROC curve for Decision Tree Model')\n",
    "plt.plot([-0.05,1.05],[-0.05,1.05],'r--',linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).6. ROC AUC Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The ROC AUC score of the Decision Tree Model ')\n",
    "print('--------------------------------------------------','\\n')\n",
    "print('ROC AUC : ',roc_auc_score(y_test,y_test_prob_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).7. Cross Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=log_loss(y_test,y_pred_test_dt)\n",
    "print('The Cross Entropy score of Decision Tree Model ')\n",
    "print('----------------------------------------------------','\\n')\n",
    "print('Cross Entropy : ',cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).8. Bias Error and Variance Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "scores=cross_val_score(estimator=dt,X=X_train,y=y_train,cv=k,scoring='accuracy')\n",
    "print(\"Mean Score : \",np.mean(scores))\n",
    "print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).9. Decision Tree Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier(max_leaf_nodes=15)\n",
    "plt.figure(figsize=(34,30))\n",
    "plot_tree(dt.fit(X_train,y_train),fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a).10. Important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imp=pd.DataFrame({'Features':X_train.columns,'Score':dt.feature_importances_})\n",
    "features_imp[features_imp['Score']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Decision Tree Model we have found out The features which contribute significantly for the model. The features are:\n",
    "\n",
    "    * creatinine_apache\n",
    "    * gcs_verbal_apache\n",
    "    * map_apache\n",
    "    * ventilated_apache\n",
    "    * apache_4a_hospital_death_prob\n",
    "    * apache_4a_icu_death_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b) Inferences for Decision Tree Classification Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Cross entropy for the Decision Tree model is 4.19\n",
    "\n",
    "\n",
    "* ROC AUC Score for the Decision Tree Model is 63.89\n",
    "\n",
    "\n",
    "* The Model Accuracy for the Decision Tree Model is coming out to be around 88%.\n",
    "\n",
    "\n",
    "* f1 weighted avg for the Decision Tree Model is around 88%.\n",
    "\n",
    "\n",
    "* Specificity : 92%\n",
    "\n",
    "\n",
    "* Sensitivity : 34%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naive Bayes Classification Model using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model using sklearn:\n",
    "NB=GaussianNB()\n",
    "NB.fit(X_train,y_train)\n",
    "y_pred_test_NB=NB.predict(X_test)\n",
    "y_pred_train_NB=NB.predict(X_train)\n",
    "y_test_prob_NB=NB.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) Model Performance Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a).1. Confusion Matrix for Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Confusion matrix:\n",
    "\n",
    "NB_confusion_mat=confusion_matrix(y_test,y_pred_test_dt)\n",
    "tn = NB_confusion_mat[0,0]\n",
    "tp = NB_confusion_mat[1,1]\n",
    "fp = NB_confusion_mat[0,1]\n",
    "fn = NB_confusion_mat[1,0]\n",
    "print('Confusion Matrix for Decision Tree Model : ')\n",
    "print('------------------------------------------------','\\n')\n",
    "print(NB_confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).2. Graphical Representation of Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(NB_confusion_mat,annot=True,linewidth=3.5,linecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).3. Sensitivity and Specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensitivity_NB=(tp/(tp+fn))\n",
    "print('Sensitivity of the Naive Bayes Model: ')\n",
    "print('-------------------------------------','\\n')\n",
    "print(Sensitivity_NB)\n",
    "print('\\n')\n",
    "Specificity_NB=(tn/(tn+fp))\n",
    "print('Specificity of the naive Bayes Model: ')\n",
    "print('-------------------------------------','\\n')\n",
    "print(Specificity_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).4. Accuracy Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Accuracy score of test data for the Naive Bayes Model: ')\n",
    "print(accuracy_score(y_test,y_pred_test_NB),'\\n')\n",
    "print('The Accuracy score of the train data for the Naive Bayes Model: ')\n",
    "print(accuracy_score(y_train,y_pred_train_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).5. Naive Bayes Model classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the classification report:\n",
    "\n",
    "NB_reporttrain=classification_report(y_train,y_pred_train_NB)\n",
    "print('Naive Bayes Model classification report for train data: ')\n",
    "print('------------------------------------------------------','\\n')\n",
    "print(NB_reporttrain)\n",
    "NB_reporttest=classification_report(y_test,y_pred_test_NB)\n",
    "print('Naive Bayes Model classification report for test data: ')\n",
    "print('------------------------------------------------------','\\n')\n",
    "print(NB_reporttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).6. ROC Curve for Naive Bayes Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,th=roc_curve(y_test,y_test_prob_NB)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr,tpr,color='pink')\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.grid()\n",
    "plt.title('ROC curve for Decision Tree Model')\n",
    "plt.plot([-0.05,1.05],[-0.05,1.05],'b--',linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).7. ROC AUC Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The ROC AUC score of the Decision Tree Model ')\n",
    "print('--------------------------------------------------','\\n')\n",
    "print('ROC AUC : ',roc_auc_score(y_test,y_test_prob_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).8. Cross Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropyNB=log_loss(y_test,y_pred_test_NB)\n",
    "print('The Cross Entropy score of Naive Bayes Model ')\n",
    "print('----------------------------------------------------','\\n')\n",
    "print('Cross Entropy : ',cross_entropyNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).9. Bias Error and Variance Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "scores=cross_val_score(estimator=NB,X=X_train,y=y_train,cv=k,scoring='accuracy')\n",
    "print(\"Mean Score : \",np.mean(scores))\n",
    "print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                  Overall Inference:         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular representation of derived Inferences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the structure of the table:\n",
    "\n",
    "columns=['Overall Accuracy Score','Accuracy for train data','Accuracy for test data','Specificity',\n",
    "         'Sensitivity','f1 score weighted avg','ROC AUC Score','Cross Entropy']\n",
    "indexes=['Logistic Regresion Model','K-Nearest Neighbors Model','Descision Tree Model','Naive Bayes Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataframe for compariosn of Models:\n",
    "model_data=pd.DataFrame(columns=columns,index=indexes)\n",
    "\n",
    "# Inserting observed readings to the data fields:\n",
    "model_data['Overall Accuracy Score']=['92%','91%','88%','83%']\n",
    "model_data['Accuracy for train data']=['91.66%','93.01%','99.90%','83.47%']\n",
    "model_data['Accuracy for test data']=['91.96%','90.28%','87.86%','83.24%']\n",
    "model_data['Specificity']=['98.52%','98.44%','88%','92.80%']\n",
    "model_data['Sensitivity']=['20.74%','10.34%','34.19%','34%']\n",
    "model_data['f1 score weighted avg']=['90%','91%','88%','86%']\n",
    "model_data['ROC AUC Score']=[79.55,66.86,63.89,77.95]\n",
    "model_data['Bias Error']=['8.37','9.12','12.35','16.51']\n",
    "model_data['Variance Error']=['42.05','31.38','42.54','66.10']\n",
    "model_data['Cross Entropy']=[2.77,3.10,4.19,5.7]\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences based on comparison of Base Models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Overall Accuracy Score: \n",
    "\n",
    "    1. Logistic Regression Model has the highest overall accuracy of about 92%.\n",
    "\n",
    "    2. Decision Tree Model yeilds the lowest overall model accuracy of about 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Overfitting/Underfitting:\n",
    "\n",
    "    1. All the classification models exhibit overfitting of the trained data with respect to the test data.\n",
    "\n",
    "    2. The model accuracy for train data and test data for both Logistic Regression Model and K-Nearest Neighbors Model has very less overfitting.\n",
    "    \n",
    "    3. As observed,the model accuracy for train data and test data for the Decision Tree Model has a considerably high difference in accuracies which can be considered a high overfitting condition in comparison to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. f1 score weighted avg:\n",
    "\n",
    "    1. The Logistic Regression Model has the highest weighted harmonic mean between precison and recall of about 91% based on the classification report.\n",
    "    \n",
    "    2. Both K-Nearest Neighbors Model and Descision Tree Model have nearly similar weighted harmonic mean between precison and recall of about 88%.\n",
    "    \n",
    "    3. We can further look at the recall values of the positive class and negative class to get more insights on the specificity and sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity and Specificity:\n",
    "\n",
    "    1. Logistic Regression Model :\n",
    "      * Specificity = 98.52%\n",
    "      * Sensitivity = 20.74% \n",
    "    \n",
    "    2. K-Nearest Neigbors Model :\n",
    "      * Specificity = 98.44%\n",
    "      * Sensitivity = 10.34% \n",
    "    \n",
    "    3. Decision Tree Model :\n",
    "      * Specificity = 88%\n",
    "      * Sensitivity = 34.19% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ROC AUC Score:\n",
    "\n",
    "    1. The Logistic Regression Model has the maximum area under the ROC curve with a ROC AUC Score of 79.55.\n",
    "    \n",
    "    2. The Decision Tree Model has the minimum area under the ROC curve with a ROC AUC Score of 63.89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Cross Entropy:\n",
    "\n",
    "    1. Minimum Cross Entropy score for Logistic Regression model : \n",
    "    H_logistic(y) = 2.77\n",
    "    \n",
    "    2. Maximum Cross Entropy score for Naive Bayes Model : \n",
    "    H_Decisiontree(y) = 5.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                       MODEL OPTIMIZATION                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Classification Model using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model using sklearn:\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred_test_rf=rf.predict(X_test)\n",
    "y_pred_train_rf=rf.predict(X_train)\n",
    "y_test_prob_rf=rf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.a) Model Performance Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a).1. Confusion Matrix for Random Forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Confusion matrix:\n",
    "\n",
    "rf_confusion_mat=confusion_matrix(y_test,y_pred_test_rf)\n",
    "tn = rf_confusion_mat[0,0]\n",
    "tp = rf_confusion_mat[1,1]\n",
    "fp = rf_confusion_mat[0,1]\n",
    "fn = rf_confusion_mat[1,0]\n",
    "print('Confusion Matrix for Random Forest Model : ')\n",
    "print('------------------------------------------------','\\n')\n",
    "print(rf_confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a).2. Graphical Representation of Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(rf_confusion_mat,annot=True,linewidth=3.5,linecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a).3. Sensitivity and Specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensitivity_random_forest=(tp/(tp+fn))\n",
    "print('Sensitivity of the Decision Tree Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Sensitivity_random_forest)\n",
    "print('\\n')\n",
    "Specificity_random_forest=(tn/(tn+fp))\n",
    "print('Specificity of the Decision Tree Model: ')\n",
    "print('---------------------------------------------','\\n')\n",
    "print(Specificity_random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a).4. Accuracy Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Accuracy score of test data for the Random Forest Model: ')\n",
    "print(accuracy_score(y_test,y_pred_test_rf),'\\n')\n",
    "print('The Accuracy score of the train data for the Random Forest Model: ')\n",
    "print(accuracy_score(y_train,y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).5. Random Forest Model classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the classification report:\n",
    "\n",
    "rf_reporttrain=classification_report(y_train,y_pred_train_rf)\n",
    "print('Random Forest Model classification report for train data: ')\n",
    "print('---------------------------------------------------------','\\n')\n",
    "print(rf_reporttrain)\n",
    "rf_report=classification_report(y_test,y_pred_test_rf)\n",
    "print('Random Forest Model classification report for test data: ')\n",
    "print('--------------------------------------------------------','\\n')\n",
    "print(rf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).6. ROC Curve for Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,th=roc_curve(y_test,y_test_prob_rf)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr,tpr,color='magenta')\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.grid()\n",
    "plt.title('ROC curve for Random Forest Model')\n",
    "plt.plot([-0.05,1.05],[-0.05,1.05],'r--',linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).7. ROC AUC Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The ROC AUC score of the Random Forest Model ')\n",
    "print('--------------------------------------------------','\\n')\n",
    "print('ROC AUC : ',roc_auc_score(y_test,y_test_prob_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).8. Cross Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=log_loss(y_test,y_pred_test_rf)\n",
    "print('The Cross Entropy score of Random Forest Model ')\n",
    "print('----------------------------------------------------','\\n')\n",
    "print('Cross Entropy : ',cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a).9. Bias Error and Variance Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "scores=cross_val_score(estimator=rf,X=X_train,y=y_train,cv=k,scoring='accuracy')\n",
    "print(\"Mean Score : \",np.mean(scores))\n",
    "print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning Base models to find out best parameters for further optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each base model has been chosen and a set of parameters has been considered for fine tuning.\n",
    "Using these set of parameters we have used ``Grid Search Cross Validation`` technique for Hyperparameter tunning the models where the Cross Validation considered is 10-Fold Cross Validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_params=[{'penalty':['l1','l2','elasticnet'],'max_iter':[100, 110, 120, 150, 180, 200],\n",
    "             }]\n",
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "grid_dt=GridSearchCV(estimator=LogisticRegression(),\n",
    "                     param_grid=tune_params,cv=k,scoring='f1_weighted')\n",
    "grid_model=grid_dt.fit(X_train,y_train)\n",
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the Decision Tree Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_params=[{'criterion':['entropy', 'gini'],'splitter':['best','random'],\n",
    " 'max_depth': np.arange(0, 200, 10),\n",
    " 'min_samples_split':[2, 5, 8, 10, 20]}]\n",
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "grid_dt=GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                     param_grid=tune_params,cv=k,scoring='f1_weighted')\n",
    "grid_model=grid_dt.fit(X_train,y_train)\n",
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the KNNeighbours Classification Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_params=[{'n_neighbors':[5,6,7,8],'weights':['uniform','distance'],\n",
    " 'algorithm':['auto'],\n",
    " 'leaf_size':np.arange(30, 50, 10)}]\n",
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "grid_dt=GridSearchCV(estimator=KNeighborsClassifier(),\n",
    "                     param_grid=tune_params,cv=k,scoring='f1_weighted')\n",
    "grid_model=grid_dt.fit(X_train,y_train)\n",
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning the Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_params=[{'criterion':['entropy', 'gini'],\n",
    "    'n_estimators': [100,200,400,800,1200],'random_state':[10],\n",
    " 'max_depth': np.arange(0, 100, 10),'min_samples_leaf': [1, 2, 4, 6],\n",
    " 'min_samples_split':[2, 5, 10, 20]}]\n",
    "k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "grid_dt=GridSearchCV(estimator=RandomForestClassifier(),\n",
    "                     param_grid=tune_params,cv=k,scoring='f1_weighted')\n",
    "grid_model=grid_dt.fit(X_train,y_train)\n",
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refitting the Tunned Based models to compare the performances: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunned_basemodels(model):\n",
    "    for j in model:\n",
    "        plt.figure(figsize=(9,8))\n",
    "        Tuned_base_model=j.fit(X_train,y_train)\n",
    "        ypred_train=Tuned_base_model.predict(X_train)\n",
    "        ypred_test=Tuned_base_model.predict(X_test)\n",
    "        ypred_test_prob=Tuned_base_model.predict_proba(X_test)\n",
    "        print(f'Overall accuracy of {j} for resampled train data : ',\n",
    "        np.round(accuracy_score(y_train,ypred_train),2)*100)\n",
    "        print(f'Overall accuracy of {j} for resampled test data : ',\n",
    "        np.round(accuracy_score(y_test,ypred_test),2)*100,'\\n')\n",
    "        confusion_mat=confusion_matrix(y_test,ypred_test)\n",
    "        tn = confusion_mat[0,0]\n",
    "        tp = confusion_mat[1,1]\n",
    "        fp = confusion_mat[0,1]\n",
    "        fn = confusion_mat[1,0]\n",
    "        print(f'Confusion Matrix for {j} Model : ')\n",
    "        print(confusion_mat,'\\n')\n",
    "        Sensitivity=((tp/(tp+fn))*100)\n",
    "        print('Sensitivity : ',np.round(Sensitivity,2))\n",
    "        Specificity=((tn/(tn+fp))*100)\n",
    "        print('Specificity : ',np.round(Specificity,2),'\\n')\n",
    "        print(f'Classification report of {j} for resampled train data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(y_train,ypred_train),'\\n')\n",
    "        print(f'Classification report of {j} for resampled test data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(y_test,ypred_test),'\\n')\n",
    "        fpr,trp,th=roc_curve(y_test,ypred_test_prob[:,1])\n",
    "        plt.plot(fpr,trp,color='brown',linewidth=3.5)\n",
    "        plt.xlim([-0.05,1.05])\n",
    "        plt.ylim([-0.05,1.05])\n",
    "        plt.grid()\n",
    "        plt.xlabel('FPR(1-Specificity)',fontsize=15)\n",
    "        plt.ylabel('TPR(Sensitivity)',fontsize=15)\n",
    "        plt.plot([-0.05,1.05],[-0.05,1.05],'--b')\n",
    "        plt.show()\n",
    "        print(f'AUC Score of {j} for resampled test data : ')\n",
    "        print(roc_auc_score(y_test,ypred_test_prob[:,1]),'\\n')\n",
    "        cross_entropy=log_loss(y_test,ypred_test)\n",
    "        print(f'The Cross Entropy score of {j} Model : ',cross_entropy,'\\n')\n",
    "        k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "        scores=cross_val_score(estimator=j,X=X_train,y=y_train,cv=k,scoring='accuracy')\n",
    "        print(\"Mean Score : \",np.mean(scores))\n",
    "        print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "        print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')\n",
    "        print('----------------------------------------------------------------------------------------------------------------------------','\\n')\n",
    "models=[LogisticRegression(max_iter=200,penalty='l2'),DecisionTreeClassifier(criterion='entropy',\n",
    " max_depth=10,min_samples_split=8,splitter='random'),\n",
    "        KNeighborsClassifier(algorithm='auto',leaf_size=30,n_neighbors=6,weights='distance'),\n",
    "        GaussianNB(),RandomForestClassifier()]\n",
    "tunned_basemodels(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, Decision tree and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking presence of Imbalance in the target variable (hospital_death) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking % of subcatgories in the Target variable(hospital_death): ','\\n')\n",
    "print('% of Positive class(0) : ',)\n",
    "print((patient_records['hospital_death'].value_counts()[0]/patient_records['hospital_death'].shape[0])*100,'\\n')\n",
    "print('% of Negative class(1) : ',)\n",
    "print((patient_records['hospital_death'].value_counts()[1]/patient_records['hospital_death'].shape[0])*100)\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(patient_records['hospital_death'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that their is imbalance present in the target variable (hospital_death).\n",
    "\n",
    "* 91.37% of 0 class which is the majority class.\n",
    "* 8.63% of 1 class which is the minority class.\n",
    "\n",
    "Presence of class imbalance in the target feature can be a defect which might result in faulty predictions by the classification algorithm with a tendency to classify the testing data more towards the majority class.\n",
    "\n",
    "\n",
    "Thus, in order to remove Imablance we will use SMOTE analysis or the Synthetic Minority OverSampling Technique by which new examples can be synthesized from the existing examples. This type of data augmentation for the minority class can reduce the huge difference between the % distribution of each unique sub-category in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing SMOTE to reduce imbalance:\n",
    " \n",
    "smote=SMOTE(sampling_strategy=0.95,random_state=10)\n",
    "X_res, y_res = smote.fit_resample(X,y)\n",
    "print('Resampled X :',X_res.shape)\n",
    "print('Resampled y :',y_res.shape,'\\n')\n",
    "# Checking the resampled data:\n",
    "print('Checking % of subcatgories in the Target variable(y) after SMOTE analysis: ','\\n')\n",
    "print('% of Positive class(0) : ',np.round(((y_res['hospital_death'].value_counts()[0]/y_res['hospital_death'].shape[0])*100),2))\n",
    "print('% of Negative class(1) : ',np.round((y_res['hospital_death'].value_counts()[1]/y_res['hospital_death'].shape[0])*100,2),'\\n')\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(y_res['hospital_death'])\n",
    "\n",
    "# Splitting the dataset into train and test in the ratio 80:20\n",
    "rs=10\n",
    "XRes_train,XRes_test,yRes_train,yRes_test=train_test_split(X_res,y_res,test_size=0.2,random_state=rs)\n",
    "print('XRes_train :',XRes_train.shape)\n",
    "print('yRes_train :',yRes_train.shape)\n",
    "print('XRes_test :',XRes_test.shape)\n",
    "print('yRes_test :',yRes_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refitting Logistic Regression, Decision tree, Random Forest algorithm on Resampled data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Model using sklearn on resampled datset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_forest_classifier(model):\n",
    "    for j in model:\n",
    "        res_base_models=j.fit(XRes_train,yRes_train)\n",
    "        yRes_pred_test_rf=res_base_models.predict(XRes_test)\n",
    "        yRes_pred_train_rf=res_base_models.predict(XRes_train)\n",
    "        yRes_test_prob_rf=res_base_models.predict_proba(XRes_test)[:,1]\n",
    "        print(f'The Accuracy score of resampled test data for the {j} Model: ')\n",
    "        print(accuracy_score(yRes_test,yRes_pred_test_rf))\n",
    "        print(f'The Accuracy score of resampled train data for the {j} Model: ')\n",
    "        print(accuracy_score(yRes_train,yRes_pred_train_rf),'\\n')\n",
    "        rfRes_confusion_mat=confusion_matrix(yRes_test,yRes_pred_test_rf)\n",
    "        tn = rfRes_confusion_mat[0,0]\n",
    "        tp = rfRes_confusion_mat[1,1]\n",
    "        fp = rfRes_confusion_mat[0,1]\n",
    "        fn = rfRes_confusion_mat[1,0]\n",
    "        print(f'Confusion Matrix of {j} Model for resampled test data : ')\n",
    "        print('-----------------------------------------------------------------','\\n')\n",
    "        print(rfRes_confusion_mat,'\\n')\n",
    "        rfRes_reporttrain=classification_report(yRes_train,yRes_pred_train_rf)\n",
    "        print(f'{j} Model classification report after resampling for train data: ')\n",
    "        print('------------------------------------------------------------------------','\\n')\n",
    "        print(rfRes_reporttrain,'\\n')\n",
    "        rfRes_reporttest=classification_report(yRes_test,yRes_pred_test_rf)\n",
    "        print(f'{j} Model classification report after resampling for test data: ')\n",
    "        print('------------------------------------------------------------------------','\\n')\n",
    "        print(rfRes_reporttest,'\\n')\n",
    "        Sensitivity=(tp/(tp+fn))\n",
    "        print('Sensitivity : ',Sensitivity)\n",
    "        Specificity=(tn/(tn+fp))\n",
    "        print('Specificity : ',Specificity,'\\n')\n",
    "        print(f'AUC score of the {j} Model for resampled test data : ',\n",
    "              roc_auc_score(yRes_test,yRes_test_prob_rf),'\\n')\n",
    "        cross_entropy=log_loss(yRes_test,yRes_pred_test_rf)\n",
    "        print(f'The Cross Entropy score of {j} Model : ',cross_entropy,'\\n')\n",
    "        k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "        scores=cross_val_score(estimator=j,X=XRes_train,y=yRes_train,cv=k,scoring='accuracy')\n",
    "        print(\"Mean Score : \",np.mean(scores))\n",
    "        print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "        print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')\n",
    "        print('------------------------------------------------------------------------------------------------------------------------------------------------------------','\\n')\n",
    "model=[LogisticRegression(max_iter=200,penalty='l2'),\n",
    "        DecisionTreeClassifier(criterion='entropy',\n",
    " max_depth=10,min_samples_split=8,splitter='random'),RandomForestClassifier()]\n",
    "Random_forest_classifier(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Random Forest Model given by the function :  ``Random_forest_classifier(model)`` has better Precision and Recall than the previous model built with actual training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe=RFE(estimator=RandomForestClassifier(),n_features_to_select=None,step=1,verbose=0)\n",
    "rfe_model=rfe.fit(XRes_train,yRes_train)\n",
    "features_imp=pd.DataFrame({'Features':X_res.columns,\n",
    "                           'Rank':rfe_model.ranking_}).sort_values(by='Rank',ascending=True)\n",
    "print(features_imp[features_imp['Rank']==1],'\\n'=;1\n",
    "XRes_new=features_imp[features_imp['Rank']==1]\n",
    "print(list(XRes_new['Features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables are obtained after Recurcise feature elimination:\n",
    "\n",
    "    map_apache', 'apache_4a_icu_death_prob', 'apache_2_diagnosis', 'apache_3j_diagnosis', 'apache_4a_hospital_death_prob', 'bun_apache', 'creatinine_apache', 'wbc_apache', 'gcs_verbal_apache', 'glucose_apache', 'diabetes_mellitus', 'resprate_apache'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refitting the Logistic Regression model, Decision Tree model, Naive Bayes and Random Forest model on the new dataset containing features obtained after recursive elimination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_imp_features(model):\n",
    "    for j in model:\n",
    "        X_new=X[['map_apache', 'apache_4a_icu_death_prob', 'apache_2_diagnosis', \n",
    "                 'apache_3j_diagnosis', 'apache_4a_hospital_death_prob', \n",
    "                 'bun_apache', 'creatinine_apache', 'wbc_apache', \n",
    "                 'gcs_verbal_apache', 'glucose_apache', 'diabetes_mellitus', 'resprate_apache']]\n",
    "        Y=patient_records['hospital_death']\n",
    "        smt=SMOTE(sampling_strategy=0.95,random_state=10)\n",
    "        Xnew_res, Y_res = smt.fit_resample(X_new,Y)\n",
    "        XRes_train,XRes_test,YRes_train,YRes_test=train_test_split(Xnew_res,Y_res,test_size=0.2,random_state=10)\n",
    "        plt.figure(figsize=(9,8))\n",
    "        Res_new_model=j.fit(XRes_train,YRes_train)\n",
    "        YRespred_train=Res_new_model.predict(XRes_train)\n",
    "        YRespred_test=Res_new_model.predict(XRes_test)\n",
    "        YRespred_test_prob=Res_new_model.predict_proba(XRes_test)\n",
    "        print(f'Overall accuracy of {j} for resampled train data : ',\n",
    "        np.round(accuracy_score(YRes_train,YRespred_train),2)*100)\n",
    "        print(f'Overall accuracy of {j} for resampled test data : ',\n",
    "        np.round(accuracy_score(YRes_test,YRespred_test),2)*100,'\\n')\n",
    "        confusion_mat=confusion_matrix(YRes_test,YRespred_test)\n",
    "        tn = confusion_mat[0,0]\n",
    "        tp = confusion_mat[1,1]\n",
    "        fp = confusion_mat[0,1]\n",
    "        fn = confusion_mat[1,0]\n",
    "        print(f'Confusion Matrix for {j} Model : ')\n",
    "        print(confusion_mat,'\\n')\n",
    "        Sensitivity=(tp/(tp+fn))\n",
    "        print('Sensitivity : ',Sensitivity)\n",
    "        Specificity=(tn/(tn+fp))\n",
    "        print('Specificity : ',Specificity,'\\n')\n",
    "        print(f'Classification report of {j} for resampled train data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(YRes_train,YRespred_train),'\\n')\n",
    "        print(f'Classification report of {j} for resampled test data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(YRes_test,YRespred_test),'\\n')\n",
    "        fpr,trp,th=roc_curve(YRes_test,YRespred_test_prob[:,1])\n",
    "        plt.plot(fpr,trp,color='brown',linewidth=3.5)\n",
    "        plt.xlim([-0.05,1.05])\n",
    "        plt.ylim([-0.05,1.05])\n",
    "        plt.grid()\n",
    "        plt.xlabel('FPR(1-Specificity)',fontsize=15)\n",
    "        plt.ylabel('TPR(Sensitivity)',fontsize=15)\n",
    "        plt.plot([-0.05,1.05],[-0.05,1.05],'--b')\n",
    "        plt.show()\n",
    "        print(f'AUC Score of {j} for resampled test data : ')\n",
    "        print(roc_auc_score(YRes_test,YRespred_test_prob[:,1]),'\\n')\n",
    "        cross_entropy=log_loss(YRes_test,YRespred_test)\n",
    "        print(f'The Cross Entropy score of {j} Model : ',cross_entropy,'\\n')\n",
    "        k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "        scores=cross_val_score(estimator=j,X=XRes_train,y=YRes_train,cv=k,scoring='accuracy')\n",
    "        print(\"Mean Score : \",np.mean(scores))\n",
    "        print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "        print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')\n",
    "        print('----------------------------------------------------------------------------------------------------------------------------','\\n')\n",
    "models=[LogisticRegression(max_iter=200,penalty='l2'),\n",
    "        DecisionTreeClassifier(criterion='entropy',\n",
    " max_depth=10,min_samples_split=8,splitter='random'),RandomForestClassifier()]\n",
    "model_imp_features(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis it can be inferred that :\n",
    "    \n",
    "    * The Sensitivity and Specificity of both Logistic Regression model and Decision Tree model has increased significantly from the base models resulting in an improved weighted-avgerage of Precision and Recall rates.\n",
    "    \n",
    "    * The Decision Tree Model has a better overall performance than the Logistic Model for the new dataset obtained after recursive feature elimination and resampling the data.\n",
    "    \n",
    "    * The Random Forest Model has not shown any major improvement from the previous model but has the best overall performance as compared to the Logistic Regression and Decision Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we will be considering the Random Forest Classifier model.\n",
    "\n",
    "Random Forest Model :\n",
    "    \n",
    "    * Overall accuracy of RandomForestClassifier() for resampled train data : 100.00\n",
    "    * Overall accuracy of RandomForestClassifier() for resampled test data : 94.0\n",
    "    * Sensitivity : 0.97\n",
    "    * Specificity : 0.92\n",
    "    * f1-score weighted average : 94%\n",
    "    * AUC Score of RandomForestClassifier() for resampled test data : 0.9842\n",
    "    * Bias Error : 6.316\n",
    "    * variance error : 0.2168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms on the selected features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AdaBoost Model\n",
    "* GradientBoost Model\n",
    "* XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_imp_boosting(model):\n",
    "    for j in model:\n",
    "        X_new=X[['map_apache', 'apache_4a_icu_death_prob', 'apache_2_diagnosis', \n",
    "                 'apache_3j_diagnosis', 'apache_4a_hospital_death_prob', \n",
    "                 'bun_apache', 'creatinine_apache', 'wbc_apache', \n",
    "                 'gcs_verbal_apache', 'glucose_apache', 'diabetes_mellitus', 'resprate_apache']]\n",
    "        Y=patient_records['hospital_death']\n",
    "        smt=SMOTE(sampling_strategy=0.95,random_state=10)\n",
    "        Xnew_res, Y_res = smt.fit_resample(X_new,Y)\n",
    "        XRes_train,XRes_test,YRes_train,YRes_test=train_test_split(Xnew_res,Y_res,test_size=0.2,random_state=10)\n",
    "        plt.figure(figsize=(9,8))\n",
    "        Res_model=j.fit(XRes_train,YRes_train)\n",
    "        YRespred_train=Res_model.predict(XRes_train)\n",
    "        YRespred_test=Res_model.predict(XRes_test)\n",
    "        YRespred_test_prob=Res_model.predict_proba(XRes_test)\n",
    "        print(f'Overall accuracy of {j} for resampled train data : ',\n",
    "        np.round(accuracy_score(YRes_train,YRespred_train),2)*100)\n",
    "        print(f'Overall accuracy of {j} for resampled test data : ',\n",
    "        np.round(accuracy_score(YRes_test,YRespred_test),2)*100,'\\n')\n",
    "        confusion_mat=confusion_matrix(YRes_test,YRespred_test)\n",
    "        tn = confusion_mat[0,0]\n",
    "        tp = confusion_mat[1,1]\n",
    "        fp = confusion_mat[0,1]\n",
    "        fn = confusion_mat[1,0]\n",
    "        print(f'Confusion Matrix for {j} Model : ')\n",
    "        print(confusion_mat,'\\n')\n",
    "        Sensitivity=(tp/(tp+fn))\n",
    "        print('Sensitivity : ',Sensitivity)\n",
    "        Specificity=(tn/(tn+fp))\n",
    "        print('Specificity : ',Specificity,'\\n')\n",
    "        print(f'Classification report of {j} for resampled train data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(YRes_train,YRespred_train),'\\n')\n",
    "        print(f'Classification report of {j} for resampled test data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(YRes_test,YRespred_test),'\\n')\n",
    "        fpr,trp,th=roc_curve(YRes_test,YRespred_test_prob[:,1])\n",
    "        plt.plot(fpr,trp,color='red')\n",
    "        plt.xlim([-0.05,1.05])\n",
    "        plt.ylim([-0.05,1.05])\n",
    "        plt.grid()\n",
    "        plt.xlabel('FPR(1-Specificity)',fontsize=15)\n",
    "        plt.ylabel('TPR(Sensitivity)',fontsize=15)\n",
    "        plt.plot([-0.05,1.05],[-0.05,1.05],'--b')\n",
    "        plt.show()\n",
    "        print(f'AUC Score for {j} for resampled test data : ')\n",
    "        print(roc_auc_score(YRes_test,YRespred_test_prob[:,1]),'\\n')\n",
    "        cross_entropy=log_loss(YRes_test,YRespred_test)\n",
    "        print(f'The Cross Entropy score of {j} Model : ',cross_entropy,'\\n')\n",
    "        k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "        scores=cross_val_score(estimator=j,X=XRes_train,y=YRes_train,cv=k,scoring='accuracy')\n",
    "        print(\"Mean Score : \",np.mean(scores))\n",
    "        print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "        print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')\n",
    "        print('----------------------------------------------------------------------------------------------------------------------------','\\n')\n",
    "boosting_classifier=[AdaBoostClassifier(),GradientBoostingClassifier(),\n",
    "                     XGBClassifier(eval_metric='logloss')]\n",
    "model_imp_boosting(boosting_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms on complete Resampled data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AdaBoost Model\n",
    "* GradientBoost Model\n",
    "* XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Res_boosting_algorithm(model):\n",
    "    for j in model:\n",
    "        plt.figure(figsize=(9,8))\n",
    "        Res_xgboost_model=j.fit(XRes_train,yRes_train)\n",
    "        yRespred_train=Res_xgboost_model.predict(XRes_train)\n",
    "        yRespred_test=Res_xgboost_model.predict(XRes_test)\n",
    "        yRespred_test_prob=Res_xgboost_model.predict_proba(XRes_test)\n",
    "        print(f'Overall accuracy of {j} for resampled train data : ',\n",
    "        np.round(accuracy_score(yRes_train,yRespred_train),2)*100)\n",
    "        print(f'Overall accuracy of {j} for resampled test data : ',\n",
    "        np.round(accuracy_score(yRes_test,yRespred_test),2)*100,'\\n')\n",
    "        confusion_mat=confusion_matrix(yRes_test,yRespred_test)\n",
    "        tn = confusion_mat[0,0]\n",
    "        tp = confusion_mat[1,1]\n",
    "        fp = confusion_mat[0,1]\n",
    "        fn = confusion_mat[1,0]\n",
    "        print(f'Confusion Matrix for {j} Model : ')\n",
    "        print(confusion_mat,'\\n')\n",
    "        Sensitivity=(tp/(tp+fn))\n",
    "        print('Sensitivity : ',Sensitivity)\n",
    "        Specificity=(tn/(tn+fp))\n",
    "        print('Specificity : ',Specificity,'\\n')\n",
    "        print(f'Classification report of {j} for resampled train data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(yRes_train,yRespred_train),'\\n')\n",
    "        print(f'Classification report of {j} for resampled test data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(yRes_test,yRespred_test),'\\n')\n",
    "        fpr,trp,th=roc_curve(yRes_test,yRespred_test_prob[:,1])\n",
    "        plt.plot(fpr,trp,color='red')\n",
    "        plt.xlim([-0.05,1.05])\n",
    "        plt.ylim([-0.05,1.05])\n",
    "        plt.grid()\n",
    "        plt.xlabel('FPR(1-Specificity)',fontsize=15)\n",
    "        plt.ylabel('TPR(Sensitivity)',fontsize=15)\n",
    "        plt.plot([-0.05,1.05],[-0.05,1.05],'--b')\n",
    "        plt.show()\n",
    "        print(f'AUC Score for {j} for resampled test data : ')\n",
    "        print(roc_auc_score(yRes_test,yRespred_test_prob[:,1]),'\\n')\n",
    "        cross_entropy=log_loss(yRes_test,yRespred_test)\n",
    "        print(f'The Cross Entropy score of {j} Model : ',cross_entropy,'\\n')\n",
    "        k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "        scores=cross_val_score(estimator=j,X=XRes_train,y=yRes_train,cv=k,scoring='accuracy')\n",
    "        print(\"Mean Score : \",np.mean(scores))\n",
    "        print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "        print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')\n",
    "        print('----------------------------------------------------------------------------------------------------------------------------','\\n')\n",
    "boosting_classifier=[AdaBoostClassifier(),GradientBoostingClassifier(),\n",
    "                     XGBClassifier(eval_metric='logloss')]\n",
    "Res_boosting_algorithm(boosting_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the structure of the table:\n",
    "indexes=['Random Forest Model','AdaBoost Model','GradientBoost Model',\n",
    "                     'XGBoost Model']\n",
    "#  Dataframe for compariosn of Optimization Models:\n",
    "optModel_data=pd.DataFrame(index=indexes)\n",
    "# Inserting observed readings to the data fields:\n",
    "optModel_data['Overall Accuracy']=['95%','86%','89%','96%']\n",
    "optModel_data['Accuracy for train data']=['99.60%','86%','89%','95%']\n",
    "optModel_data['Accuracy for test data']=['93.41%','86%','88%','95%']\n",
    "optModel_data['Specificity']=['92.08%','84.82%','87.06%','96.24%']\n",
    "optModel_data['Sensitivity']=['94.82%','86.29%','89.65%','93.31%']\n",
    "optModel_data['f1 score weighted avg']=['93%','86%','89%','96%']\n",
    "optModel_data['ROC AUC Score']=[0.98,0.93,0.95,0.98]\n",
    "optModel_data['Bias Error']=[6.76,14.15,11.46,5.17]\n",
    "optModel_data['Variance Error']=[0.27,0.24,0.30,0.22]\n",
    "optModel_data['Cross Entropy']=[2.27,4.99,4.03,1.78]\n",
    "optModel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the comparison we can definitely conclude that:\n",
    "    \n",
    "    * XGBoost Model and Random Forest Model have the approx  overall accuracy of 95% on test data.\n",
    "    * XGBoost has less overfitting of train data as compared to Random Forest Model.\n",
    "    * XGBoost has a slightly lesser Bias error and variance error than Random Forest Model.\n",
    "    * f1-score weighted for XGBoost Model : 96%\n",
    "    * AUC score for XGBoost Model : 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Prediction Model (patient_survival_Prediction_XGBoostModel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_survival_Prediction_XGBoostModel(fin_model):\n",
    "        X_new=X[['map_apache', 'apache_4a_icu_death_prob', 'apache_2_diagnosis', \n",
    "                 'apache_3j_diagnosis', 'apache_4a_hospital_death_prob', \n",
    "                 'bun_apache', 'creatinine_apache', 'wbc_apache', \n",
    "                 'gcs_verbal_apache', 'glucose_apache', 'diabetes_mellitus', 'resprate_apache']]\n",
    "        Y=patient_records['hospital_death']\n",
    "        smt=SMOTE(sampling_strategy=0.95,random_state=10)\n",
    "        Xnew_res, Y_res = smt.fit_resample(X_new,Y)\n",
    "        XRes_train,XRes_test,YRes_train,YRes_test=train_test_split(Xnew_res,Y_res,test_size=0.2,random_state=10)\n",
    "        plt.figure(figsize=(10,9))\n",
    "        Res_model=fin_model.fit(XRes_train,YRes_train)\n",
    "        YRespred_train=Res_model.predict(XRes_train)\n",
    "        YRespred_test=Res_model.predict(XRes_test)\n",
    "        YRespred_test_prob=Res_model.predict_proba(XRes_test)\n",
    "        print('Overall accuracy of XGBoost Model for resampled train data : ',\n",
    "        np.round(accuracy_score(YRes_train,YRespred_train),2)*100)\n",
    "        print('Overall accuracy of XGBoost Model for resampled test data : ',\n",
    "        np.round(accuracy_score(YRes_test,YRespred_test),2)*100,'\\n')\n",
    "        confusion_mat=confusion_matrix(YRes_test,YRespred_test)\n",
    "        tn = confusion_mat[0,0]\n",
    "        tp = confusion_mat[1,1]\n",
    "        fp = confusion_mat[0,1]\n",
    "        fn = confusion_mat[1,0]\n",
    "        print('Confusion Matrix for XGBoost Model Model : ')\n",
    "        print(confusion_mat,'\\n')\n",
    "        Sensitivity=((tp/(tp+fn))*100)\n",
    "        print('Sensitivity : ',np.round(Sensitivity,2))\n",
    "        Specificity=((tn/(tn+fp))*100)\n",
    "        print('Specificity : ',np.round(Specificity,2),'\\n')\n",
    "        print('Classification report of XGBoost Model for resampled train data : ')\n",
    "        print('-------------------------------------------------------------------------------','\\n')\n",
    "        print(classification_report(YRes_train,YRespred_train),'\\n')\n",
    "        print('Classification report of XGBoost Model for resampled test data : ')\n",
    "        print('----------------------------------------------------------------','\\n')\n",
    "        print(classification_report(YRes_test,YRespred_test),'\\n')\n",
    "        fpr,trp,th=roc_curve(YRes_test,YRespred_test_prob[:,1])\n",
    "        plt.plot(fpr,trp,color='orange',linewidth=6.5)\n",
    "        plt.xlim([-0.05,1.05])\n",
    "        plt.ylim([-0.05,1.05])\n",
    "        plt.grid()\n",
    "        plt.title('ROC Curve for XGBoost Model',fontsize=15)\n",
    "        plt.xlabel('FPR(1-Specificity)',fontsize=15)\n",
    "        plt.ylabel('TPR(Sensitivity)',fontsize=15)\n",
    "        plt.plot([-0.05,1.05],[-0.05,1.05],'--r',linewidth=4.5)\n",
    "        plt.show()\n",
    "        print('AUC Score for XGBoost Model for resampled test data : ')\n",
    "        print(roc_auc_score(YRes_test,YRespred_test_prob[:,1]),'\\n')\n",
    "        cross_entropy=log_loss(YRes_test,YRespred_test)\n",
    "        print('The Cross Entropy score of XGBoost Model : ',cross_entropy,'\\n')\n",
    "        k=KFold(n_splits=10,shuffle=True, random_state=10)\n",
    "        scores=cross_val_score(estimator=fin_model,X=XRes_train,y=YRes_train,cv=k,scoring='accuracy')\n",
    "        print(\"Mean Score : \",np.mean(scores))\n",
    "        print(\"Bias error : \",(1-np.mean(scores))*100)\n",
    "        print(\"Variance error : \",(np.std(scores)/np.mean(scores))*100,'\\n')\n",
    "        print('----------------------------------------------------------------------------------------------------------------------------','\\n')\n",
    "fin_model=XGBClassifier(eval_metric='logloss')\n",
    "patient_survival_Prediction_XGBoostModel(fin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
